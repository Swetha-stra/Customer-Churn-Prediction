{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: Load and Clean Dataset",
   "id": "a7e553280de8a5f2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.173066Z",
     "start_time": "2025-12-08T17:48:36.080683Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load raw data\n",
    "df = pd.read_csv(r\"C:\\Users\\USER\\PycharmProjects\\coursework_ML\\data\\raw\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "# Convert TotalCharges to numeric (blank strings â†’ NaN)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Fill missing TotalCharges with MonthlyCharges * tenure\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'] * df['tenure'])\n",
    "\n",
    "# Drop any remaining missing rows just in case\n",
    "df = df.dropna(subset=['TotalCharges'])"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5178b0fefb5951f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Feature Engineering",
   "id": "8fa5878de9b375da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.213657Z",
     "start_time": "2025-12-08T17:48:36.180080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Estimated lifetime value\n",
    "df['estimated_ltv'] = df['MonthlyCharges'] * df['tenure']\n",
    "\n",
    "# Binary flags for internet and phone\n",
    "df['has_internet'] = (df['InternetService'] != 'No').astype(int)\n",
    "df['has_phone'] = (df['PhoneService'] == 'Yes').astype(int)\n",
    "\n",
    "# Tenure groups\n",
    "df['tenure_group'] = pd.cut(df['tenure'],\n",
    "                            bins=[0,12,24,48,60,72],\n",
    "                            labels=['0-12','12-24','24-48','48-60','60-72'])\n"
   ],
   "id": "f261125d20a5d0f5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "723457cd335bac0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 3: Separate Features and Target",
   "id": "ef2bcd0fd4efcf2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.290423Z",
     "start_time": "2025-12-08T17:48:36.265211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Target variable\n",
    "y = (df['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Drop columns not needed\n",
    "X = df.drop(columns=['customerID','Churn'])\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = ['tenure','MonthlyCharges','TotalCharges','estimated_ltv']\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "# Ensure numeric columns are float\n",
    "X[numeric_features] = X[numeric_features].astype(float)"
   ],
   "id": "d25ed00270e9691a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "31bc286bb00dcf9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 4: Preprocessing Pipelines",
   "id": "1ace2faa95e5c830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.386692Z",
     "start_time": "2025-12-08T17:48:36.348549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_features),\n",
    "    ('cat', cat_pipeline, categorical_features)\n",
    "])"
   ],
   "id": "6ee6073a5951bd06",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cfb6b020007efb49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 5: Train-Test Split",
   "id": "ecd65550617af0f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.588941Z",
     "start_time": "2025-12-08T17:48:36.451539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ],
   "id": "f40617bd84856104",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d8da46b59079aeb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 6: Fit & Transform Preprocessor",
   "id": "f9664e841653f59d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:36.820941Z",
     "start_time": "2025-12-08T17:48:36.654226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit only on training set\n",
    "preprocessor.fit(X_train_raw)\n",
    "\n",
    "# Transform train and test sets\n",
    "X_train = preprocessor.transform(X_train_raw)\n",
    "X_test = preprocessor.transform(X_test_raw)\n"
   ],
   "id": "2dc7fb90d5e23662",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "232427b69c62384e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 7: Save Preprocessor and Processed Data",
   "id": "c0ac32174f22046d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:37.045128Z",
     "start_time": "2025-12-08T17:48:36.890250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "models_path = os.path.join(project_root, 'models')\n",
    "processed_path = os.path.join(project_root, 'data', 'processed')\n",
    "\n",
    "# Create directories if not exist\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, os.path.join(models_path, 'preprocessor.joblib'))\n",
    "\n",
    "# Save processed datasets\n",
    "np.save(os.path.join(processed_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(processed_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(processed_path, 'y_train.npy'), y_train.to_numpy())\n",
    "np.save(os.path.join(processed_path, 'y_test.npy'), y_test.to_numpy())\n",
    "\n",
    "print(\"Preprocessing completed. Processed datasets and preprocessor saved.\")"
   ],
   "id": "89f6c2d54011d9f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Processed datasets and preprocessor saved.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:37.264567Z",
     "start_time": "2025-12-08T17:48:37.113395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_csv_path = os.path.join(project_root, 'data', 'processed', 'cleaned_data.csv')\n",
    "df.to_csv(processed_csv_path, index=False)\n"
   ],
   "id": "b3a9d0ca2a26cf5b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:48:37.355707Z",
     "start_time": "2025-12-08T17:48:37.330470Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7f508219b2e82779",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
